{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b4f505",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d57089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from LSTM_implementation import LSTMModel\n",
    "import re\n",
    "from typing import List\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d222062",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/NusaX-sentiment/train.csv')\n",
    "val = pd.read_csv('dataset/NusaX-sentiment/valid.csv')\n",
    "test = pd.read_csv('dataset/NusaX-sentiment/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "232abafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>219</td>\n",
       "      <td>Nikmati cicilan 0% hingga 12 bulan untuk pemes...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>209</td>\n",
       "      <td>Kue-kue yang disajikan bikin saya bernostalgia...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>436</td>\n",
       "      <td>Ibu pernah bekerja di grab indonesia</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>394</td>\n",
       "      <td>Paling suka banget makan siang di sini ayam sa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>592</td>\n",
       "      <td>Pelayanan bus DAMRI sangat baik</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text     label\n",
       "0  219  Nikmati cicilan 0% hingga 12 bulan untuk pemes...   neutral\n",
       "1  209  Kue-kue yang disajikan bikin saya bernostalgia...  positive\n",
       "2  436               Ibu pernah bekerja di grab indonesia   neutral\n",
       "3  394  Paling suka banget makan siang di sini ayam sa...  positive\n",
       "4  592                    Pelayanan bus DAMRI sangat baik  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d54cdc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      500 non-null    int64 \n",
      " 1   text    500 non-null    object\n",
      " 2   label   500 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 11.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6accf6a",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd761882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenization:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_words: int = 4000,\n",
    "                 max_sequence_length: int = 100,\n",
    "                 oov_token: str = \"<OOV>\",\n",
    "                 padding: str = 'post',\n",
    "                 truncating: str = 'post'):\n",
    "\n",
    "        self.max_words = max_words\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.oov_token = oov_token\n",
    "        self.padding = padding\n",
    "        self.truncating = truncating\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = Tokenizer(\n",
    "            num_words=max_words,\n",
    "            oov_token=oov_token,\n",
    "            filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "        )\n",
    "        \n",
    "        # Attributes to be set during fitting\n",
    "        self.vocab_size = None\n",
    "        self.word_index = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # Store preprocessing statistics\n",
    "        self.text_stats = {\n",
    "            'original_lengths': [],\n",
    "            'cleaned_lengths': [],\n",
    "            'total_texts': 0,\n",
    "            'unique_words': 0\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Store original length\n",
    "        original_length = len(text.split())\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove mentions and hashtags (but keep the text part)\n",
    "        text = re.sub(r'[@#]\\w+', '', text)\n",
    "        \n",
    "        # Remove numbers (optional - you might want to keep them)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove special characters but keep Indonesian characters and spaces\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Remove very short words (less than 2 characters)\n",
    "        words = text.split()\n",
    "        words = [word for word in words if len(word) >= 2]\n",
    "        text = ' '.join(words)\n",
    "        \n",
    "        # Store cleaned length\n",
    "        cleaned_length = len(text.split())\n",
    "        self.text_stats['original_lengths'].append(original_length)\n",
    "        self.text_stats['cleaned_lengths'].append(cleaned_length)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_texts(self, texts: List[str]) -> List[str]:\n",
    "        \n",
    "        cleaned_texts = []\n",
    "        for i, text in enumerate(texts):\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                print(f\"Processed {i}/{len(texts)} texts\")\n",
    "            \n",
    "            cleaned_text = self.clean_text(text)\n",
    "            cleaned_texts.append(cleaned_text)\n",
    "        \n",
    "        self.text_stats['total_texts'] += len(texts)\n",
    "        print(f\"Preprocessing completed. Total texts processed: {self.text_stats['total_texts']}\")\n",
    "        \n",
    "        return cleaned_texts\n",
    "    \n",
    "    def fit(self, texts: List[str]) -> 'TextTokenization':\n",
    "        \n",
    "        # Preprocess texts\n",
    "        cleaned_texts = self.preprocess_texts(texts)\n",
    "        \n",
    "        # Fit tokenizer\n",
    "        self.tokenizer.fit_on_texts(cleaned_texts)\n",
    "        \n",
    "        # Store tokenizer information\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "        self.vocab_size = min(len(self.word_index) + 1, self.max_words)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        # Calculate statistics\n",
    "        self.text_stats['unique_words'] = len(self.word_index)\n",
    "        \n",
    "        print(f\"Tokenizer fitted successfully!\")\n",
    "        print(f\"- Total unique words: {len(self.word_index)}\")\n",
    "        print(f\"- Vocabulary size (with limit): {self.vocab_size}\")\n",
    "        print(f\"- OOV token: {self.oov_token}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts: List[str]) -> np.ndarray:\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Tokenizer must be fitted before transforming. Call fit() first.\")\n",
    "        \n",
    "        print(f\"Transforming {len(texts)} texts to sequences...\")\n",
    "        \n",
    "        # Preprocess texts\n",
    "        cleaned_texts = self.preprocess_texts(texts)\n",
    "        \n",
    "        # Convert to sequences\n",
    "        sequences = self.tokenizer.texts_to_sequences(cleaned_texts)\n",
    "        \n",
    "        # Pad sequences\n",
    "        padded_sequences = pad_sequences(\n",
    "            sequences,\n",
    "            maxlen=self.max_sequence_length,\n",
    "            padding=self.padding,\n",
    "            truncating=self.truncating\n",
    "        )\n",
    "        \n",
    "        print(f\"Transformation completed. Output shape: {padded_sequences.shape}\")\n",
    "        \n",
    "        return padded_sequences\n",
    "    \n",
    "    def fit_transform(self, texts: List[str]) -> np.ndarray:\n",
    "        \n",
    "        return self.fit(texts).transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd279472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label unique values: ['neutral' 'positive' 'negative']\n",
      "Number of unique labels: 3\n",
      "Preprocessing training data...\n",
      "Preprocessing completed. Total texts processed: 500\n",
      "Tokenizer fitted successfully!\n",
      "- Total unique words: 2732\n",
      "- Vocabulary size (with limit): 2733\n",
      "- OOV token: <OOV>\n",
      "Transforming 500 texts to sequences...\n",
      "Preprocessing completed. Total texts processed: 1000\n",
      "Transformation completed. Output shape: (500, 1000)\n",
      "Preprocessing validation data...\n",
      "Transforming 100 texts to sequences...\n",
      "Preprocessing completed. Total texts processed: 1100\n",
      "Transformation completed. Output shape: (100, 1000)\n",
      "Preprocessing test data...\n",
      "Transforming 400 texts to sequences...\n",
      "Preprocessing completed. Total texts processed: 1500\n",
      "Transformation completed. Output shape: (400, 1000)\n",
      "\n",
      "Data shapes and types:\n",
      "X_train: (500, 1000), dtype: int32\n",
      "y_train: (500,), dtype: int32\n",
      "X_val: (100, 1000), dtype: int32\n",
      "y_val: (100,), dtype: int32\n",
      "X_test: (400, 1000), dtype: int32\n",
      "y_test: (400,), dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(\"Label unique values:\", train['label'].unique())\n",
    "print(\"Number of unique labels:\", len(train['label'].unique()))\n",
    "\n",
    "tokenizer = TextTokenization(max_words=10000, max_sequence_length=1000)\n",
    "encoder = LabelEncoder()\n",
    "# Tokenization and encoding\n",
    "def preprocess_dataset(df, tokenizer, encoder, is_train=False):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if is_train:\n",
    "        # For training data, fit tokenizer and encoder\n",
    "        X = tokenizer.fit_transform(df_copy['text'].tolist())\n",
    "        encoder.fit(df_copy['label'])\n",
    "        y = encoder.transform(df_copy['label'])\n",
    "    else:\n",
    "        # For validation/test data, only transform\n",
    "        X = tokenizer.transform(df_copy['text'].tolist())\n",
    "        y = encoder.transform(df_copy['label'])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "print(\"Preprocessing training data...\")\n",
    "X_train, y_train = preprocess_dataset(train, tokenizer, encoder, is_train=True)\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "X_val, y_val = preprocess_dataset(val, tokenizer, encoder, is_train=False)\n",
    "\n",
    "print(\"Preprocessing test data...\")\n",
    "X_test, y_test = preprocess_dataset(test, tokenizer, encoder, is_train=False)\n",
    "\n",
    "# Verify data types and shapes\n",
    "print(f\"\\nData shapes and types:\")\n",
    "print(f\"X_train: {X_train.shape}, dtype: {X_train.dtype}\")\n",
    "print(f\"y_train: {y_train.shape}, dtype: {y_train.dtype}\")\n",
    "print(f\"X_val: {X_val.shape}, dtype: {X_val.dtype}\")\n",
    "print(f\"y_val: {y_val.shape}, dtype: {y_val.dtype}\")\n",
    "print(f\"X_test: {X_test.shape}, dtype: {X_test.dtype}\")\n",
    "print(f\"y_test: {y_test.shape}, dtype: {y_test.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30ad50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Eksperimen 1: Pengaruh Jumlah Layer LSTM ===\n",
      "\n",
      "Training LSTM with 1 layers...\n",
      "Epoch 1/10 - train_loss: 1.0855 - train_acc: 0.3840 - val_loss: 1.0797 - val_acc: 0.3800\n",
      "Epoch 2/10 - train_loss: 1.0789 - train_acc: 0.3500 - val_loss: 1.0779 - val_acc: 0.3800\n",
      "Epoch 3/10 - train_loss: 1.0809 - train_acc: 0.3780 - val_loss: 1.0781 - val_acc: 0.3800\n",
      "Epoch 4/10 - train_loss: 1.0829 - train_acc: 0.3840 - val_loss: 1.0786 - val_acc: 0.3800\n",
      "Epoch 5/10 - train_loss: 1.0818 - train_acc: 0.3700 - val_loss: 1.0790 - val_acc: 0.3800\n",
      "Epoch 6/10 - train_loss: 1.0805 - train_acc: 0.3760 - val_loss: 1.0785 - val_acc: 0.3800\n",
      "Epoch 7/10 - train_loss: 1.0801 - train_acc: 0.3840 - val_loss: 1.0783 - val_acc: 0.3800\n",
      "Epoch 8/10 - train_loss: 1.0774 - train_acc: 0.3840 - val_loss: 1.0779 - val_acc: 0.3800\n",
      "Epoch 9/10 - train_loss: 1.0774 - train_acc: 0.3780 - val_loss: 1.0779 - val_acc: 0.3800\n",
      "Epoch 10/10 - train_loss: 1.0782 - train_acc: 0.3880 - val_loss: 1.0779 - val_acc: 0.3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhinto\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dhinto\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Dhinto\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 1 layers:\n",
      "Test Accuracy: 0.3825\n",
      "Macro F1-Score: 0.1844\n",
      "\n",
      "Training LSTM with 2 layers...\n",
      "Epoch 1/10 - train_loss: 1.0736 - train_acc: 0.4100 - val_loss: 1.1119 - val_acc: 0.3800\n",
      "Epoch 2/10 - train_loss: 1.0823 - train_acc: 0.4000 - val_loss: 1.0849 - val_acc: 0.3800\n",
      "Epoch 3/10 - train_loss: 1.0814 - train_acc: 0.3780 - val_loss: 1.0782 - val_acc: 0.3800\n",
      "Epoch 4/10 - train_loss: 1.0786 - train_acc: 0.3720 - val_loss: 1.0779 - val_acc: 0.3800\n",
      "Epoch 5/10 - train_loss: 1.0795 - train_acc: 0.3840 - val_loss: 1.0780 - val_acc: 0.3800\n",
      "Epoch 6/10 - train_loss: 1.0779 - train_acc: 0.3780 - val_loss: 1.0782 - val_acc: 0.3800\n",
      "Epoch 7/10 - train_loss: 1.0804 - train_acc: 0.3780 - val_loss: 1.0781 - val_acc: 0.3800\n",
      "Epoch 8/10 - train_loss: 1.0799 - train_acc: 0.3840 - val_loss: 1.0785 - val_acc: 0.3800\n",
      "Epoch 9/10 - train_loss: 1.0797 - train_acc: 0.3840 - val_loss: 1.0782 - val_acc: 0.3800\n",
      "Epoch 10/10 - train_loss: 1.0830 - train_acc: 0.3780 - val_loss: 1.0795 - val_acc: 0.3800\n"
     ]
    }
   ],
   "source": [
    "# Eksperimen 1: Pengaruh Jumlah Layer LSTM\n",
    "print(\"\\n=== Eksperimen 1: Pengaruh Jumlah Layer LSTM ===\")\n",
    "\n",
    "lstm_layer_variations = [1, 2, 3]\n",
    "lstm_layer_results = []\n",
    "\n",
    "for num_layers in lstm_layer_variations:\n",
    "    print(f\"\\nTraining LSTM with {num_layers} layers...\")\n",
    "    \n",
    "    model = LSTMModel(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        embedding_dim=32,\n",
    "        lstm_units=16,\n",
    "        num_classes=len(encoder.classes_),\n",
    "        num_lstm_layers=num_layers,\n",
    "        bidirectional=False,  # Unidirectional untuk eksperimen ini\n",
    "        dropout_rate=0.3,\n",
    "        learning_rate=0.01\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Evaluasi\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    lstm_layer_results.append({\n",
    "        'num_layers': num_layers,\n",
    "        'history': history,\n",
    "        'test_results': results,\n",
    "        'model': model\n",
    "    })\n",
    "    \n",
    "    print(f\"Results for {num_layers} layers:\")\n",
    "    print(f\"Test Accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"Macro F1-Score: {results['macro_f1_score']:.4f}\")\n",
    "\n",
    "# Plot hasil eksperimen 1\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "for result in lstm_layer_results:\n",
    "    plt.plot(result['history']['train_loss'], label=f\"{result['num_layers']} layers\")\n",
    "plt.title('Training Loss vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "for result in lstm_layer_results:\n",
    "    plt.plot(result['history']['val_loss'], label=f\"{result['num_layers']} layers\")\n",
    "plt.title('Validation Loss vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "f1_scores = [result['test_results']['macro_f1_score'] for result in lstm_layer_results]\n",
    "layer_counts = [result['num_layers'] for result in lstm_layer_results]\n",
    "plt.bar(layer_counts, f1_scores)\n",
    "plt.title('Macro F1-Score by Number of LSTM Layers')\n",
    "plt.xlabel('Number of LSTM Layers')\n",
    "plt.ylabel('Macro F1-Score')\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
